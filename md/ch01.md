

# Portfolio Statistics and Optimization

chris bemis

August 11, 2023

![](2173c0102e23a5ff29d90d4353fc0339_img.jpg)

## Chapter 1

## Introduction

Equity anomalies are abundant. This statement is both terribly, and purposely, imprecise. What defines an equity anomaly? In what sense are they anomalous? And just how abundant is abundant?

Eugene Fama in 1970 wrote *Efficient Capital Markets* [9], a pronouncement that the stock market efficiently disseminates every piece of information available about every stock that is traded into their respective prices. Asserting that no amount of analysis can separate future winners from losers, for, all available information has already been incorporated into the current price.

Broadly speaking, the above is the efficient market hypothesis (EMH). In the EMH world, no amount of fundamental analysis (reading balance sheets, income statements, and cash flow statements) or technical analysis (identifying patterns, real or perceived) will generate excess returns in the long run. Any contradiction to this is an anomaly. Again we are left with an imprecise statement as to what is meant by excess returns.

Consider, for example, the chart in Figure 1. Here a one dollar investment is made in each of two securities; the blue line being a market proxy, Standard and Poor's S&P 500 Index. At the end of a ten year period, there is clear 'excess.' But to what end? There is, even without a proper statistical analysis something unappealing in the wild swings and variations in the purported winner. Our sense of caution is not wrong, either, since in this case the winner is simply a leveraged version of the market—returns are  $2\times$  the un-levered version. This surely can't be what is meant by the impossibility of generating excess returns, and in fact it isn't.

However, a somewhat technical treatment is needed. Referring to the figure once more, there is something *riskier* in the leveraged series. Defining risk and its requisite properties has become a seminal issue over the past few decades. At the outset, volatility was tantamount to risk. And as we will see in subsequent chapters, much of the theoretical machinations supporting the efficient market hypothesis will imply that excess returns are a positive function of risk (read volatility). The example above notwithstanding, this is antithetical to what is observed empirically and what will be shown in subsequent chapters. In fact

### An Apparent Excess Return

![Line graph titled 'An Apparent Excess Return'. The X-axis represents Date (2006 to 2015) and the Y-axis represents Value of $1 Invested (0.5 to 3.0). Two lines are plotted: a blue line representing the S&P 500 and a red line representing a related security. Both lines show significant volatility, with the red line generally tracking above the blue line, suggesting superior performance, especially after 2010.](e94f3bbb6f7501b9a1344dd0210e5dd8_img.jpg)

Line graph titled 'An Apparent Excess Return'. The X-axis represents Date (2006 to 2015) and the Y-axis represents Value of \$1 Invested (0.5 to 3.0). Two lines are plotted: a blue line representing the S&P 500 and a red line representing a related security. Both lines show significant volatility, with the red line generally tracking above the blue line, suggesting superior performance, especially after 2010.

Figure 1.1: Value of \$1 invested in the S&P 500 [25], including dividends and corporate actions, over a ten year period in blue. The red line shows the value of \$1 invested in a related security. Performance is superior, but not without drawbacks.

we will exhibit that the market, rather than compensating for risk in the equity markets, punishes risk-holders across almost every measurement.

Staying within the confines of the EMH world, then, what should one do if choosing a good stock is proscribed by theoretical fiat? Buy them all, of course! One of the pivotal contributions to the field in fact belies this very statement. At its heart is a treatment of systemic and idiosyncratic risk which culminates in a familiar refrain: you can't beat the market, and diversification is the key to reducing idiosyncratic risk.

On the one hand, then, we have a yet-to-be defined model for determining anomalous returns, and on the other, an again yet-to-be defined model to reduce risk across a set of securities.

We will often attempt to gain a deeper understanding of the central themes of modern finance through the lens of the so-called *value* anomaly. We do this for two reasons: first, we choose to take most market standard models as providing a taxonomy rather than as prescriptive of actual market behavior; and second, while much of our work centers on using and utilizing statistical features of past price performance, we lean towards a belief that these do not encapsulate all of the information in the market. To quote Andy Redleaf, the founder of Whitebox Advisors, in his book, *Panic* [26]:

*A market in which traders can predict future price moves solely on the basis of past price moves is pathetic. Traders can only do this because public financial markets are price-obsessed and price-paranoid. Financial markets are awash in price information (how many times a second does some price on some board somewhere change?) and relatively devoid of knowledge about value and cost to compare it to. Traders are desperate to know what everyone else is paying precisely because there is so little information about what anyone **should** be paying.*

We arrive next at the focus of the present work: statistics and optimization, with an emphasis on interpretation and allocation. The formulation of the efficient market hypothesis in this book will be a statistical one, with some modification of the standard literature. While we will focus primarily on Merton and Sharpe’s Capital Asset Pricing Model [22, 31] and the subsequent Fama-French three factor model [10], we will often eschew their language of risk. Instead, we will use these two models much as the Black-Scholes’ option formula is in practice: for an interpretive tool to understand asset allocation and for uniform comparisons.

The Capital Asset Pricing Model (or simply CAPM) is, in essence, simply a relationship between a given stock’s returns and the market’s returns (modulo something called the risk-free rate). Formally it says

$$r_t - r_f = \beta(m_t - r_f) + \epsilon_t,$$

or that the excess return for a given asset at time  $t$ ,  $r_t$ , over the risk free rate,  $r_f$ , is a constant multiple  $\beta$  times the excess return of the market,  $m_t$ , over the risk free rate, with some allowance for random variation,  $\epsilon_t$ . There are several theoretical consequences to the model, many of which will be discussed later. Presently, though, we obtain that the model gives a single variable by which to compare assets.

Returning to our first example of a leveraged market portfolio outperforming the market portfolio, we may, without calculations conclude that the former had a  $\beta$  of 2, while the latter had a  $\beta$  equal to 1, capturing quite nicely the intuition used to construct the example. The effects of leverage, correlation and risk-as-volatility are summarily presented in one metric, allowing uniform comparisons and a common language across any number of securities.

Caveats abound in the model above, however. Notably that equity returns are not stationary; i.e., the statistical properties needed to obtain a quantity like  $\beta$  above are not the same over various time slices. Calculating a stock’s  $\beta$  from 1990 to 2000 will be different than from 2000 to 2010. Or worse, six months prior to, say, the financial crisis of 2008 will differ from six months after. Worse yet, even as we will develop the framework for understanding the statistical distribution of a variable like  $\beta$  (noting that in fact most financial data should be taken as ranges and not point estimates), the movements of the market can violate the assumptions for well-behaved estimators. Put another way, even if

we make an expected range for  $\beta$ , the market’s moves could go outside this range in a heartbeat.

As David Viniar, the CFO at Goldman Sachs during the Quant Crisis of August 2007 said, “[We saw] things that were 25-standard deviation moves, several days in a row.” [15] We aren’t yet to the point where we can evaluate the sheer magnitude of this statement of improbability, but suffice it to say, it’ll defy comprehension on a universal scale.

The empirical facts of the last thirty years drive our emphasis of interpretive abilities rather than on the normative modeling that has been a feature of mathematical finance since its inception. To quote Karl Popper

No amount of observations of white swans can allow the conclusion that all swans are white, but the observation of a single black swan is sufficient to refute the conclusion.

There have been a few black swans of note, to the point that they deserve primacy in our applications of financial models. The trend within the field seems to be to accommodate new findings in ever expanding complexity, however.

An example—but one that is meritus—is the Fama-French extension to CAPM [10]. Here, finding that small companies outperform larger companies, and that higher value stocks outperform lower, they expand CAPM to

$$r_t - r_f = \beta_m(m_t - r_f) + \beta_h h_t + \beta_v v_t + \epsilon_t,$$

with  $h_t$  being the returns to a long-short portfolio that is long small companies and short large, and  $v_t$  similarly constructed from high book-to-price companies versus low book-to-price. Not surprisingly, the anomalies (relative to CAPM) that Fama and French noted are subsumed in their model, and *ipso facto*, size and value are no longer anomalies.

An implication in the standard literature is that these new factors are *risk factors*. Our treatment will not subscribe to this view as there is little empirical evidence to justify the term. Instead, we note the power of understanding an asset or a weighted portfolio of assets in terms of highly meaningful and understandable exposures within the market. We will also see that models like Fama-French and CAPM will lend themselves readily to portfolio allocation models where understanding the co-movements of assets is rife with estimation error.

Once we have gained an understanding of several statistical methods through the lens of mathematical finance, we will be in a position to study portfolio optimization. After developing some of the main tools for unconstrained and constrained optimization, we will find a direct connection between CAPM and Modern Portfolio Theory: Merton’s 1952 Mean-Variance Optimization (MVO) [22],

$$\begin{aligned}\min_w & \frac{1}{2} w' \Sigma w \\ & \mu' w = \mu^* \\ & 1' w = 1.\end{aligned}$$

where  $w$  is a vector of portfolio weights,  $\Sigma$  is the covariance of returns for the assets in question,  $\mu$  is their expected return, and  $\mu^*$  is some required return. No parameters are given from on high, however, so that while  $\Sigma$  and  $\mu$  are neatly expressed above, obtaining reasonable estimates that are *stable through a future time horizon* is no easy task. Rather, for most models presented, the necessary stationarity requirements are assumed, in contrast to what is observed in the markets.

A proper treatment of the allocation problem in modern portfolio theory therefore necessitates a statistical treatment as well – as has already been alluded to vis a vis the previous model discussions. Here, understanding the structure of the covariance of returns becomes paramount, and we will visit and revisit this topic throughout the text.

With a solid underpinning of statistics and Modern Portfolio Theory in place, we will also consider more recent developments in portfolio optimization; namely, coherent measures of risk, and, as will be the tone of the text, generally, develop tools for implementing several such measures in practice.

Our approach in every topic will be to arduously highlight axioms before obtaining results, with the belief that implemented models require the practitioner to understand exactly which of the underlying assumptions are wrong (but perhaps useful), and which are dangerous, and we strive to develop the tools to do just this in the pages that follow.

![](c834b9abb4ddf70e5d10641f87d5ff5b_img.jpg)

