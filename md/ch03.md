## Chapter 3

## Covariance

Ellipsoidal distributions like the multivariate normal and Student  $t$  distribution play a pivotal role in modeling financial data and at their heart is the covariance matrix,  $\Sigma$ . In this chapter, we discuss some key properties, empirical observations, and current research work.

The structure of the covariance matrix defines a language for the pairwise relationships in the market. We identify the mathematical framework for understanding correlation and covariance—namely the inner product—and utilize Cauchy-Schwarz to provide bounds on the former. We will also briefly discuss the relationship between correlation and independence and give an example regarding the normal distribution. Additional results follow from our interpretation of covariance as an inner product on the space of identified random variables including another Capital Asset Pricing Model (CAPM) vignette.

Some background in the theory of eigenvalues and eigenvectors for a general  $N \times N$  matrix, and results specific to positive definite matrices are necessary. With regards to covariance matrices, we relate the sum of eigenvalues to what we define as total variance, and define the square root of a positive definite matrix and relate this result to Monte Carlo simulations, as alluded to in the previous chapter. Finally, we introduce the concept of the condition number and provide some motivation for its value.

Empirical examples of the covariance matrix will center around the frequency of observed eigenvalues for a correlation matrix at a given time for a large number of equities. These observations will echo the literature and can be summarized as follows: the largest eigenvalue explains an inordinate amount of total variance, and its associated eigenvector (read eigenportfolio) is a proxy for the market as such, giving credence to the simple CAPM model; several other eigenvalues are related to sector specific drivers, and are fairly large as well; and a bulk of eigenvalues are near zero and have characteristics of a random distribution.

Finally, we will present the theory of copulas, an essential tool for imposing a covariance structure given a set of prescribed marginals. Our work here is not extensive, but should provide the practitioner with the theory and tools

for implementation. We provide a few words of caution, however, related to the application of the method and some of its shortcomings during, say, the financial crisis of 2008.

### 3.1 Covariance and Correlation

As before (2.20) we define the covariance of a random variable  $X \in \mathbb{R}^N$  with expectation  $\mu$  as

$$\text{Cov}(X) = \mathbb{E}((X - \mu)(X - \mu)')$$

giving an  $N \times N$  real valued matrix in the case where  $\text{Cov}(X)$  is defined, and we denote the  $(i, j)$  component of  $\text{Cov}(X)$  as  $\sigma_{ij}$ , and  $\sigma_{ii} = \text{Var}(X_i)$ .

In particular, for two real valued random variables,  $X$  and  $Y$ , we may introduce the notation  $\text{Cov}(X, Y)$  where

$$\text{Cov}(X, Y) = \mathbb{E}((X - \mu_X)(Y - \mu_Y)) \tag{3.1}$$

with  $\mu_X$  and  $\mu_Y$  being the expectation of  $X$  and  $Y$ , respectively. Relating this notation to the previous lines, we have

$$\text{Cov}(X, Y) = \sigma_{XY}.$$

It follows that there exists some  $\rho_{XY}$  such that,

$$\sigma_{XY} = \rho_{XY} \sigma_X \sigma_Y.$$

That is, the covariance between two random variables may be written as some scalar times the product of their standard deviations.

Far from being arbitrary,  $\rho_{XY}$  is a highly interpretable parameter called the *correlation* between  $X$  and  $Y$ . It is a measure of linear dependence, and is bounded between

$$-1 \le \rho_{XY} \le 1,$$

with the extremes being special cases in the relationship between  $X$  and  $Y$ .

To derive the above, we identify covariance as an *inner product*. For our purposes, we consider an inner product  $(\cdot, \cdot)$  to be a function of the cross product of a real valued vector space,  $V$ , over  $\mathbb{R}$  into  $\mathbb{R}$ ,

$$(\cdot, \cdot) : V \times V \to \mathbb{R}$$

satisfying the following criteria for  $X, Y, Z \in V$ , and  $a, b \in \mathbb{R}$ :

- *symmetry*

$$(X, Y) = (Y, X) \tag{3.2}$$

- *bilinearity*

$$\begin{aligned} (aX, Y) &= a(X, Y) \\ (X + Y, Z) &= (X, Z) + (Y, Z) \end{aligned} \tag{3.3}$$

- *non-negativity*

$$(X, X) \ge 0 \quad (3.4)$$

with equality only if  $X \equiv 0$ .

The definition also implies that for scalars  $\{a_i\}_i$  and  $\{b_j\}_j$ , and  $\{X_i\}_i$  and  $\{Y_j\}_j$  in  $V$ ,

$$\left(\sum_i a_i X_i, \sum_j b_j Y_j\right) = \sum_i a_i \sum_j b_j (X_i, Y_j).$$

This is left to the reader as an exercise.

**Example 3.1.1.** For  $V = \mathbb{R}^N$ , and  $A \in \mathbb{R}^{N \times N}$ ,

$$(x, y)_A = x' A y$$

is an inner product when  $A$  is symmetric and positive definite. This immediately follows by checking the criteria of the definition. We have

$$\begin{aligned} (x, y)_A &= x' A y \\ &= (x' A y)' \\ &= y' A' x \\ &= y' A x, \end{aligned}$$

and for  $a \in \mathbb{R}$ ,

$$\begin{aligned} (ax, y)_A &= ax' A y \\ &= a(x, y)_A \end{aligned}$$

and

$$\begin{aligned} (x + y, z)_A &= (x + y)' A z \\ &= x' A z + y' A z. \end{aligned}$$

Finally  $(x, y)_A$  is positive definite exactly when  $A$  is positive definite.

We may consider the vector space of real valued random variables over  $\mathbb{R}$ . As in (2.25), we have

$$\operatorname{Cov}(X, Y) = \mathbb{E}((X - \mu_X)(Y - \mu_Y)).$$

We have already shown in (2.21) that the expectation operator is linear, so that we have

$$\begin{aligned} \operatorname{Cov}(aX, Y) &= \mathbb{E}(a(X - \mu_X)(Y - \mu_Y)) \\ &= a\mathbb{E}((X - \mu_X)(Y - \mu_Y)) \\ &= a\operatorname{Cov}(X, Y). \end{aligned}$$

Similarly,  $Cov(\cdot, \cdot)$  is symmetric and bilinear.

We are left to show non-negativity. We have, clearly, that

$$Cov(X, X) = Var(X),$$

and we know that variance is a nonnegative number. In fact, looking at the definition of variance in (2.5), we see that the quantity is only zero when  $X \equiv \mu_X$ . That is,  $Var(X) = 0$  if and only if  $X$  is a constant random variable. This gives that covariance is an inner product on the space of real valued random variables over  $\mathbb{R}$  where constants are identified. We call such a space where a particular relation defines an identification a *quotient space*.

We may also identify when the covariance matrix of  $X \in \mathbb{R}^N$  is positive definite. We have shown already that

$$Var(w'X) = Cov(w'X, w'X) = w'\Sigma w.$$

Hence  $\Sigma$  is positive definite only if  $Var(w'X) = 0$  implies  $w'X$  is a constant. We will say that the random variable  $X$  is *linearly independent*, if  $w'X = c$ , a constant, implies  $w \equiv 0$ . From this discussion, then,  $\Sigma$  is positive definite if and only if  $X$  is linearly independent.

As we shall soon see, this criterion is easily violated when constructing the sample covariance (i.e., an unbiased estimator of  $\Sigma$  from data). Consider the case of  $N$  assets and  $T$  observations. Whenever  $N > T$ , we will obtain semidefinite sample covariance matrices as our system is underdetermined. Other issues arise and we will get to them shortly.

With the proof that  $Cov(\cdot, \cdot)$  is an inner product on the quotient space of real random variables in hand, we are ready to discuss correlation. The *Cauchy-Schwarz inequality* states that for an inner product  $(\cdot, \cdot)$ ,

$$|(u, v)| \le ||u|| \cdot ||v||, \tag{3.5}$$

with equality only if  $u = av$  for  $a$  a scalar, and where

$$||u||^2 = (u, u) \tag{3.6}$$

is defined as the *norm* of  $u$  induced by the inner product  $(\cdot, \cdot)$ . The proof of the Cauchy-Schwarz inequality is in the appendix.

In general, a norm is a function  $||\cdot||: V \to \mathbb{R}_+$  satisfying, for all  $X, Y \in V$ , and scalars  $a$ ,

- *absolute scalability*

$$||aX|| = |a| \cdot ||X|| \tag{3.7}$$

- *the triangle inequality*

$$||X + Y|| \le ||X|| + ||Y|| \tag{3.8}$$

- *non-negativity*

$$||X|| \ge 0 \tag{3.9}$$

with equality only in the case that  $X \equiv 0$ .

We leave the proof that  $||\cdot||^2 = (\cdot, \cdot)$  defines a norm to the reader.  
From the above, utilizing (3.5) we have that

$$|Cov(X, Y)| \le \sqrt{Cov(X, X)} \sqrt{Cov(Y, Y)} \\ = Var(X) \cdot Var(Y),$$

or in the previous notation

$$|\sigma_{XY}| \le \sigma_X \sigma_Y.$$

We may therefore define the *correlation between X and Y* as

$$\rho_{XY} = \frac{\sigma_{XY}}{\sigma_X \sigma_Y}. \quad (3.10)$$

We may write  $Corr(X, Y) = \rho_{XY}$  to emphasize the role of correlation as an operator. As one of the consequences of Cauchy-Schwarz, we know further that

$$-1 \le \rho_{XY} \le 1$$

with equality only in the case that  $X = aY$  for some  $a$  in the quotient space of random variables over  $\mathbb{R}$ .

We say two random variables are positively (negatively) correlated if their correlation coefficient,  $\rho$ , is positive (negative). In the case that correlation is zero, we say that these random variables are *uncorrelated*. Notice, too, that correlation is dimensionless (whereas volatility has the dimensions of the original random variable). Finally, correlation is a measure of linear dependence as we shall see in the next example which revisits a version of CAPM once more.

**Example 3.1.2.** Consider the model

$$r = \beta m + \epsilon$$

where  $m$ , and  $\epsilon$  are each random variables in  $\mathbb{R}$  satisfying

$$Cov(m, \epsilon) = 0 \\ Var(\epsilon) = \sigma_\epsilon^2 \\ Var(m) = \sigma_m^2.$$

Letting  $(\cdot, \cdot)$  denote  $Cov(\cdot, \cdot)$ , we may take the inner product of both sides with respect to  $m$  and obtain

$$(r, m) = \beta(m, m) + (\epsilon, m),$$

where we have used the bilinearity of the inner product on the right hand side. Simplifying based on our assumptions, we have

$$(r, m) = \beta(m, m),$$

or,

$$\frac{\sigma_{r,m}}{\sigma_m^2} = \beta$$

which yields

$$\beta = \rho \frac{\sigma_r}{\sigma_m} \quad (3.11)$$

with  $\rho$  the correlation between  $r$  and  $m$ .

The preceding model is the core definition of the Capital Asset Pricing Model. Notice that we haven't *estimated* any of these quantities or validated their applicability. Rather, we began with a model and obtained a consequence of our assumptions. Adding interpretation to assumptions,  $r$  represents a particular stock's returns,  $m$  the market's (whatever that may be) returns, and  $\epsilon$  the so-called idiosyncratic component of that stock.

The model says, then, that stock returns have a linear relationship with the market's returns, with some variation defined by  $\epsilon$ . Notice that we have not as yet prescribed a distribution for  $\epsilon$ . We will. When we do, this will limit the scope of the model but open us to more interpretation and analysis.

We also have not discussed the relationship between multiple assets under the model. This too will come, but the astute reader can likely derive the consequences now by generalizing the above and applying the term idiosyncratic appropriately.

The above example highlights some of the features of a model based on CAPM. In addition, we saw in (3.11) that  $\beta$ , the linear coefficient of the model, was a scalar multiple of  $\rho$ , clearly establishing that correlation is a measure of linear dependence between two random variables. It is important to point out, then, that in practice, observations of small correlation (in absolute value) do not preclude a strong relationship between two random variables; viz., a near perfect quadratic relationship may exist with a close to zero correlation.

We have already seen an unbiased estimator of variance (2.43). For covariance, we have that

$$\hat{\sigma}_{XY} = \frac{1}{N-1} \sum_{i=1}^{N} (X_i - \hat{\mu}_X)(Y_i - \hat{\mu}_Y) \quad (3.12)$$

is an unbiased estimator as well (this is left as an exercise). An estimate of correlation can be obtained from these estimators by replacing population with sample moments in (3.10); viz.,

$$\hat{\rho}_{XY} = \frac{\hat{\sigma}_{XY}}{\hat{\sigma}_X \hat{\sigma}_Y}. \quad (3.13)$$

In general, the above technique is called the *method of moments*, and for our purposes can be summarized as an estimator which equates sample moments to population moments in a formula for a given distribution parameter.

![Scatter plot showing the empirical joint distribution of log returns for IBM (Y-axis) and S&P 500 (X-axis). The X-axis ranges from approximately -0.25 to 0.15, and the Y-axis ranges from approximately -0.2 to 0.2. The data points are clustered around the origin. An orange line represents the estimated regression line, showing a positive correlation. Concentric purple ellipses are overlaid, representing the joint distribution contours.](657acccf744d33f1fc3a1652741a256e_img.jpg)

CAPM  $\beta$  Regression for IBM Using S&P 500 as a Market Proxy

Scatter plot showing the empirical joint distribution of log returns for IBM (Y-axis) and S&P 500 (X-axis). The X-axis ranges from approximately -0.25 to 0.15, and the Y-axis ranges from approximately -0.2 to 0.2. The data points are clustered around the origin. An orange line represents the estimated regression line, showing a positive correlation. Concentric purple ellipses are overlaid, representing the joint distribution contours.

Figure 3.1: Empirical joint distribution of log returns of the S&P 500 from 1980 through 2015 [25] with estimated  $\hat{\beta}$  of 1.03.

**Example 3.1.3.** The method of moments estimator of variance is

$$s_{m,N}^2 = \frac{1}{N} \sum_{i=1}^{N} (X_i - \hat{\mu})^2,$$

which we know from (2.43) is a biased estimator. Even so, it is a *consistent estimator*; i.e.,

$$s_{m,N}^2 \to \sigma^2$$

in probability.

The above example shows that a method of moments estimator may be biased (and likely is so), but consistent (and again, under light constraints likely is so). The correlation estimator presented here is both biased and consistent.

A method of moments estimator for  $\beta$  in the CAPM model above is given by

$$\hat{\beta} = \hat{\rho} \frac{\hat{\sigma}_r}{\hat{\sigma}_m}. \quad (3.14)$$

We shall see in our work on ordinary least squares (OLS) that this is exactly the OLS estimate of  $\beta$ , and that it is in fact unbiased as well.

**Example 3.1.4.** We have already seen a version of the relationship that motivates the CAPM approach in Figure 2.2. We may now calculate  $\hat{\rho}$  and  $\hat{\beta}$  using

![Line graph titled 'Rolling Window Estimator of $\hat{\beta}$'. The x-axis is 'Date' ranging from 1980 to 2015. The y-axis is $\hat{\beta}$ ranging from 0.6 to 1.2. The plot shows the rolling estimate of $\hat{\beta}$ for IBM returns based on S&P 500 returns over time, exhibiting high volatility, particularly around 1995 and 2000.](6ef03c6e28f2f1fd4f98529146ffeccc_img.jpg)

Line graph titled 'Rolling Window Estimator of \$\hat{\beta}\$'. The x-axis is 'Date' ranging from 1980 to 2015. The y-axis is \$\hat{\beta}\$ ranging from 0.6 to 1.2. The plot shows the rolling estimate of \$\hat{\beta}\$ for IBM returns based on S&P 500 returns over time, exhibiting high volatility, particularly around 1995 and 2000.

Figure 3.2: Rolling estimate of  $\hat{\beta}$  for IBM returns based on S&P 500 from 1980 through 2015.

the whole sample. We update the original figure in Figure 3.1 to show the regression line given by the estimated values below:

$$\begin{aligned}\hat{\rho} &= 0.59 \\ \hat{\sigma}_{IBM} &= 0.0787 \\ \hat{\sigma}_{SPX} &= 0.0443 \\ \hat{\beta} &= 1.03.\end{aligned}$$

Notice that the slope of the regression line is not parallel to the major axis of the ellipses defined by the covariance matrix. While this is visually jarring, it is by design as we shall see that  $\hat{\beta}$  is the result of minimizing squared errors in the y-axis dimension. Other questions arise as well:

- If we look at rolling time windows, will the results be substantially different?
- Does our estimate of  $\hat{\beta}$  change through time?
- Is it correlation or the ratio of vols that drives the variation in  $\hat{\beta}$ ?

We treat the first two points as purely optical. We see in Figure 3.1 what a rolling window using 252 trading days (approximately one year) of data yields for our estimate of  $\hat{\beta}$ .

![](96b090628713287072839afc1b077881_img.jpg)

Figure 3.3 displays two time series plots from 1980 to 2015.

The top plot, titled "Ratio of IBM Volatility to S&P Volatility," shows the Volatility Ratio (Y-axis, ranging from 1 to 3) over Date (X-axis, ranging from 1980 to 2015). The ratio fluctuates significantly, peaking around 1995 near 3.5 and dropping sharply around 2008/2009 to near 1.0.

The bottom plot, titled "IBM and S&P Correlation," shows the Correlation (Y-axis, ranging from 0.2 to 0.8) over Date (X-axis, ranging from 1980 to 2015). The correlation fluctuates around 0.75, dipping significantly around 1995/1996 to near 0.2, and recovering to around 0.75 by 2015.

Figure 3.3: Empirical joint distribution of log returns of the S&P 500 from 1980 through 2015 with estimated  $\hat{\beta}$  of 1.03.

We clearly identify that our estimate varies significantly through time. There is perhaps a mean-reverting component as well. In fact, services like Bloomberg offer up so-called shrinkage estimators of  $\beta$  that attempt to capture this very phenomenon, calculating their own estimator  $\tilde{\beta}$  as

$$\tilde{\beta} = 0.67\hat{\beta} + 0.33.$$

Clearly this would be a biased estimator of  $\beta$ . The model reflects some desirable properties of the dynamics shown however. In particular, the dynamics implied in the above are that a company's  $\beta$  to the market should center around  $\beta = 1$ .

Interestingly, the bursting of the tech bubble in early 2000 is visible in the dramatic uptick of  $\hat{\beta}$ . This is less pronounced, however, in the financial crisis in 2008. A reasonable observation is that there may be sector-specific exposures that impact a company in addition to the market as such.

Focusing on these time periods a bit more, we have yet to distinguish whether the driver of  $\hat{\beta}$  dynamics is correlation or volatility. The upper Figure 3.1 shows a plot of the ratio of estimated volatilities of IBM and the S&P, while the lower figure shows estimated correlation between the two. From a visual inspection, again, the two crises exhibit different behavior. The tech bubble shows an increase in the volatility ratio as well as an increase in correlation, while the financial crisis shows a decrease in the former and increase in the latter. In both cases, we see that in the event of a crisis, there is some evidence to expect that correlations increase between securities. This is yet another stylized feature of equity returns.

#### 3.1.1 Correlation and Independence

Correlation and independence are related but distinct properties between random variables. It is easy to show that if two univariate random variables,  $X$  and  $Y$ , are independent, then they are uncorrelated; viz.,

$$\begin{aligned}\rho_{XY} &= \mathbb{E}((X - \mu_X)(Y - \mu_Y)) \\ &= \mathbb{E}(X - \mu_X) \mathbb{E}(Y - \mu_Y) \\ &= (\mu_X - \mu_X)(\mu_Y - \mu_Y) \\ &= 0.\end{aligned}$$

The converse is not always true, however. One exception is in the case of jointly normal random variables. The proof is a bit more involved than the prior statement, however.

**Theorem 3.1.1.** (Multivariate Normality, Correlation, and Independence) If  $X \sim N(\mu_X, \Sigma_X)$  and  $Y \sim N(\mu_Y, \Sigma_Y)$  are each multivariate normal random variables which are jointly normal and uncorrelated, then  $X$  and  $Y$  are independent.

Proof. Let

$$Z = \begin{pmatrix} X \\ Y \end{pmatrix},$$

with  $X$  and  $Y$  distributed as above. Then we have  $Z \sim N(\mu, \Sigma)$ , with

$$\mu = \begin{pmatrix} \mu_X \\ \mu_Y \end{pmatrix},$$

and

$$\Sigma = \begin{pmatrix} \Sigma_X & 0 \\ 0 & \Sigma_Y \end{pmatrix}$$

since expectation is taken component-wise and the elements of  $X$  and  $Y$  are uncorrelated by assumption. We show the independence of  $X$  and  $Y$  by writing the probability density function of  $Z$  as the product of the densities for  $X$  and  $Y$ .

We have

$$\phi_{\mu,\Sigma}(z) = \frac{1}{\det(\Sigma)^{1/2}} \frac{1}{(2\pi)^{N/2}} \cdot \exp\left(-\frac{1}{2}(z-\mu)'\Sigma^{-1}(z-\mu)\right),$$

where  $N = \dim(Z) = \dim(X) + \dim(Y)$ . We will denote the dimensions of  $X$  and  $Y$  by  $N_X$  and  $N_Y$ , respectively.

By the structure of  $\Sigma$ , we have that the determinant is given by

$$\det(\Sigma) = \det(\Sigma_X) \cdot \det(\Sigma_Y)$$

and the inverse is given by

$$\Sigma^{-1} = \begin{pmatrix} \Sigma_X^{-1} & 0 \\ 0 & \Sigma_Y^{-1} \end{pmatrix}$$

Putting this together, we have, referring to the components of the density of  $Z$  in turn,

$$\frac{1}{\det(\Sigma)^{1/2}} = \frac{1}{(\det(\Sigma_X)\det(\Sigma_Y))^{1/2}} = \frac{1}{\det(\Sigma_X)^{1/2}\det(\Sigma_Y)^{1/2}},$$
$$\frac{1}{(2\pi)^{N/2}} = \frac{1}{(2\pi)^{(N_X+N_Y)/2}} = \frac{1}{(2\pi)^{N_X/2}(2\pi)^{N_Y/2}}.$$

The pattern continues. Looking now at the exponential, we have

$$\exp\left(-\frac{1}{2}(z-\mu)'\Sigma^{-1}(z-\mu)\right) = \exp\left(-\frac{1}{2}\left(\begin{pmatrix} x \\ y \end{pmatrix} - \begin{pmatrix} \mu_x \\ \mu_y \end{pmatrix}\right)'\Sigma^{-1}\left(\begin{pmatrix} x \\ y \end{pmatrix} - \begin{pmatrix} \mu_x \\ \mu_y \end{pmatrix}\right)\right) = \exp\left(-\frac{1}{2}\left(\begin{pmatrix} x-\mu_x \\ y-\mu_y \end{pmatrix}\right)'\Sigma^{-1}\left(\begin{pmatrix} x-\mu_x \\ y-\mu_y \end{pmatrix}\right)\right) = \exp\left(-\frac{1}{2}\left(\begin{pmatrix} x-\mu_x \\ y-\mu_y \end{pmatrix}\right)'\begin{pmatrix} \Sigma_X^{-1} & 0 \\ 0 & \Sigma_Y^{-1} \end{pmatrix}\begin{pmatrix} x-\mu_x \\ y-\mu_y \end{pmatrix}\right)\right) = \exp\left(-\frac{1}{2}((x-\mu_X)'\Sigma_X(x-\mu_X) + (y-\mu_Y)'\Sigma_Y(y-\mu_Y))\right) = \exp\left(-\frac{1}{2}(x-\mu_X)'\Sigma_X(x-\mu_X)\right)\exp\left(-\frac{1}{2}(y-\mu_Y)'\Sigma_Y(y-\mu_Y)\right).$$

Therefore,

$$\phi_{\mu,\Sigma}(z) = \phi_{\mu,\Sigma}(x,y) = \frac{1}{\det(\Sigma_X)^{1/2}\det(\Sigma_Y)^{1/2}} \frac{1}{(2\pi)^{N_X/2}} \frac{1}{(2\pi)^{N_Y/2}} \cdot \exp\left(-\frac{1}{2}(x-\mu_X)'\Sigma_X^{-1}(x-\mu_X)\right) \cdot \exp\left(-\frac{1}{2}(y-\mu_Y)'\Sigma_Y^{-1}(y-\mu_Y)\right) = \phi_{\mu_X,\Sigma_X}(x)\phi_{\mu_Y,\Sigma_Y}(y),$$

proving  $X$  and  $Y$  are independent.

The astute reader may have noticed that care was taken in the above example to explicitly say *jointly* normal random variables. In fact, it is not remarkable to have random variables  $X_1$  and  $X_2$ , for instance which are each normally distributed but whose joint distribution is not normal. In fact, due to a result by Sklar [32], we know that for a set of univariate random variables,  $\{X_i\}_{i=1}^N$ , with marginal distribution functions,  $F_i(\cdot)$ , any joint distribution may be constructed which respects the marginal distributions prescribed. So, for instance, it is possible to have normal marginals with a Student  $t$  joint distribution [13]. Further, this is a constructive procedure which we establish in the next section on copulas.

### 3.2 Copulas

A *copula* is the joint distribution of random variables,  $\{U_i\}_{i=1}^N$ , each of which is uniformly distributed on  $[0, 1]$ . We say that a univariate random variable is uniformly distributed on  $[0, 1]$  if

$$\mathbb{P}(U\le x)=x \tag{3.15}$$

for  $x\in[0,1]$ , and denote this by  $U\sim U([0,1])$ . Clearly in this case,  $F_U(x)=x$ , and the probability density is simply  $f_U(x)=1$ . We will denote a copula by  $C$ , and based on the above, we must have

$$C(u_1,\dots,u_N)=\mathbb{P}(U_1\le u_1,\dots,U_N\le u_N). \tag{3.16}$$

As mentioned previously, *Sklar’s Theorem* states that for any random variables,  $\{X_i\}_{i=1}^N$  with marginals  $F_i(\cdot)$  and joint distribution  $F(\cdot)$ , there exists a copula,  $C$ , such that

$$F(x_1,\dots,x_N)=C\left(F_1(x_1),\dots,F_N(x_N)\right), \tag{3.17}$$

and that if the  $F_i$ ’s are unique, then so is  $C$ . This is a powerful result, but one that can be proven readily in the continuous case. We leave to the reader to prove the fact that

$$F_i(X_i)\sim U([0,1]). \tag{3.18}$$

With this result in hand, we simply write the joint distribution in Sklar’s Theorem in terms of a copula,  $C$ , as

$$\begin{aligned} F(x_1,\dots,x_N)&=\mathbb{P}(X_1\le x_1,\dots,X_N\le x_N) \\ &=\mathbb{P}\left(F_1(X_1)\le F_1(x_1),\dots,F_N(X_N)\le F_N(x_N)\right) \\ &=\mathbb{P}\left(U_1\le F_1(x_1),\dots,U_N\le F_N(x_N)\right) \\ &=C\left(F_1(x_1),\dots,F_N(x_N)\right). \end{aligned}$$

For a specified,  $F(\cdot)$ , then, and continuous,  $F_i(\cdot)$ , we may construct a copula as

$$C\left(u_1,\dots,u_N\right)=F\left(F_1^{-1}(u_1),\dots,F_N^{-1}(u_N)\right). \tag{3.19}$$

The above result says that if we so desire, we may divorce the joint and marginal densities because the linking may be done entirely through some copula. Or, as so happens in practice, we may specify marginal distributions and a joint distribution separately.

**Example 3.2.1.** Let  $X_1$  and  $X_2$  be distributed as standard normal random variables, and let  $St_{\mu,\Sigma;\nu}(\cdot)$  be the joint distribution of a two dimensional Student  $t$  distribution with  $\nu$  degrees of freedom. Then

$$C(u_1, u_2) = St_{\mu,\Sigma;\nu}(\Phi^{-1}(u_1), \Phi^{-1}(u_2))$$

or

$$C(\Phi(x_1), \Phi(x_2)) = St_{\mu,\Sigma;\nu}(x_1, x_2)$$

exhibits a pair of random variables, jointly Student  $t$ , and with marginals that are standard normal.

Even more, for ellipsoidal distributions, we may focus solely on correlation, disregarding both position and scale; i.e., expectation and variance. This is due to the fact that copula’s have a so-called *rank-invariant property*, namely, if  $g_i(\cdot)$  are each strictly increasing functions

$$g_i: \mathbb{R} \mapsto \mathbb{R},$$

for  $i = 1, \dots, N$ , and  $C$  is the copula of  $\{X_i\}$  as in (??), then  $C$  is also the copula of  $\{g_i(X_i)\}$ .

Proof. Let  $F(\cdot)$  be the joint distribution function of  $X$ , a multivariate random variable and  $g_i(\cdot)$  strictly increasing functions from  $\mathbb{R}$  to  $\mathbb{R}$ . We know by the change of variable theorem (2.14), that the CDF of  $g_i(X_i)$  is

$$\tilde{F}_i(\cdot) = F_i \circ g_i^{-1}(\cdot) = F_i(g_i^{-1}(\cdot))$$

whose inverse is

$$\tilde{F}_i^{-1}(\cdot) = g_i \circ F_i^{-1}(\cdot) = g_i(F_i^{-1}(\cdot)).$$

Now, denoting the CDF of  $(g_1(X_1), \dots, g_N(X_N))'$  by  $F_g(\cdot)$ , we have

$$\begin{aligned} C(u_1, \dots, u_N) &= F(F_1^{-1}(u_1), \dots, F_N^{-1}(u_N)) \\ &= \mathbb{P}(X_1 \le F_1^{-1}(u_1), \dots, X_N \le F_N^{-1}(u_N)) \\ &= \mathbb{P}(g_1(X_1) \le g_1(F_1^{-1}(u_1)), \dots, g_N(X_N) \le g_N(F_N^{-1}(u_N))) \\ &= F_g(g_1(F_1^{-1}(u_1)), \dots, g_N(F_N^{-1}(u_N))). \end{aligned}$$

The result of this property is that we may strictly consider correlation structures when modeling ellipsoidal joint distributions via a copula rather than covariance structures; consider

$$g_i: X_i \mapsto \frac{X_i - \mu_i}{\sigma_i}$$

for finite mean and standard deviation,  $\mu_i$  and  $\sigma_i$ .

![Four contour plots showing simulated joint distributions. The plots are arranged in a 2x2 grid. The x and y axes range from -0.4 to 0.4. The contours represent different correlation parameters: -0.8 (upper left), -0.2 (upper right), 0.2 (lower left), and 0.8 (lower right). The contours are elongated ellipses, indicating the dependence structure.](119006c9a930f46881484f2229d3b927_img.jpg)

Four contour plots showing simulated joint distributions. The plots are arranged in a 2x2 grid. The x and y axes range from -0.4 to 0.4. The contours represent different correlation parameters: -0.8 (upper left), -0.2 (upper right), 0.2 (lower left), and 0.8 (lower right). The contours are elongated ellipses, indicating the dependence structure.

Figure 3.4: Simulated joint distributions using a copula approach, assuming a multivariate normal distribution with correlation parameter ranging from -0.8, -0.2, 0.2, and 0.8, clockwise from upper left subplot. In each case, the marginal distribution is fixed.

**Example 3.2.2.** Revisiting the example of random variables, jointly Student  $t$ , with standard normal marginals, we may use the preceding result to write

$$\begin{aligned}C(u_1, u_2) &= St_{\mu, \Sigma; \nu}(\Phi^{-1}(u_1), \Phi^{-1}(u_2)) \\&= St_{0, R; \nu}\left(\frac{\Phi^{-1}(u_1) - \mu_1}{\sigma_1}, \frac{\Phi^{-1}(u_2) - \mu_2}{\sigma_2}\right)\end{aligned}$$

for  $R$  the correlation matrix obtained from  $\Sigma$ .

**Example 3.2.3.** In practice,  $F(\cdot)$  and marginals,  $\{F_i(\cdot)\}$  are determined *a priori*; i.e., in a manner fixing a model. Oftentimes, copulas are used to simulate data with these prescribed distributions. Here, we look at a simple case of

![Two histograms showing simulated marginal distributions for S&P and IBM monthly log returns. Both distributions are fitted with a Student t distribution curve (red line). The S&P distribution is centered around 0, ranging from -0.3 to 0.3. The IBM distribution is centered around 0, ranging from -0.4 to 0.6.](a5401b088b668d0554e8e96ea558121c_img.jpg)

Two histograms showing simulated marginal distributions for S&P and IBM monthly log returns. Both distributions are fitted with a Student t distribution curve (red line). The S&P distribution is centered around 0, ranging from -0.3 to 0.3. The IBM distribution is centered around 0, ranging from -0.4 to 0.6.

Figure 3.5: Simulated marginal distributions fit to a Student  $t$  distribution with  $\nu = 5$  degrees of freedom. The marginals shown here were input into various copula functions in the preceding figure.

simulating jointly normal data with Student  $t$  marginals. Specifically, let

$$X_1 \sim St(\mu_1, \sigma_1^2; 5)$$
$$X_2 \sim St(\mu_2, \sigma_2^2; 5),$$

and let  $F(X) = \Phi_{\mu, \Sigma}$ . We know from the above that, without loss of generality, we may construct our copula function  $C(u_1, u_2) = F(F_1^{-1}(u_1), F_2^{-1}(u_2))$ ; or, if we would like, directly from the correlation matrix.

Since the joint distribution is assumed to be normal, we may first simulate  $N \times 2$  independent samples from a standard normal random variable to obtain a matrix  $\hat{Z}_0 \in \mathbb{R}^{N \times 2}$ . We know from previous work that

$$\hat{Z} = \Lambda \hat{Z}_0$$

will then be sampled according to  $N(0, \Sigma)$  when  $\Sigma = \Lambda \Lambda'$ . Notice that each column of  $\hat{Z}$  is sampled according to a standard normal. Letting  $\hat{Z}_i$  be the  $i$ th column of  $\hat{Z}$ , we set

$$\hat{U}_i = \Phi(\hat{Z}_i)$$

with evaluation occurring componentwise. From the above, each  $\hat{U}_i$  is now sampled according to a uniform random variable on  $[0, 1]$  with the joint distribution

specified by  $F(\cdot)$ . Taking inverses of the sampled uniform distributions, then,

$$\hat{X}_1 = St_{\mu_1, \sigma_1^2; 5}^{-1}(\hat{U}_1)$$
$$\hat{X}_2 = St_{\mu_2, \sigma_2^2; 5}^{-1}(\hat{U}_2),$$

we obtain our desired result: each of  $X_1$  and  $X_2$  has a Student  $t$  distribution with specified mean and variance, and the joint distribution is multivariate normal with correlation specified by  $\Sigma$ .

Figure 3.2 shows the isocontours of various joint distributions, with correlation varying over -0.8, -0.2, 0.2, and 0.8. In each case, the marginals are sampled from the same distribution. Representative histograms are shown in Figure 3.2.3.

Copulas are capable of modeling a considerable amount of information more than simple correlation. Even so, the applications that dominate the field involve using copula models to imbue marginal distributions with a given correlation structure. As our primary focus is on ellipsoidal distributions, their introduction within the chapter on covariance is not an accident and is in line with the view of most market practitioners.

The copula approach found wide appeal in credit derivatives markets due to a paper published by David Li in the Journal of Fixed Income [21]. *On Default Correlation: A Copula Function Approach* modeled default correlation in a novel way, linking marginal default risks obtained from credit default swap (CDS) pricing through a copula with a very simple structure to imply a joint distribution of credit defaults. The copula that became widely used and whose parameter eventually became a quoted market price was a multivariate normal copula with a covariance (correlation) matrix given by

$$\begin{pmatrix} 1 & \rho & \dots & \rho \\ \rho & 1 & \dots & \rho \\ \vdots & & \ddots & \\ \rho & \dots & \dots & 1 \end{pmatrix}.$$

Much like the versions of the Capital Asset Pricing Model we have seen already, the above model does two things: it provides a simplification of market relationships via market pricing and normative relationships, and produces an interpretable parameter.

Li's formula, however, is far more dangerous than Merton or Sharpe's. The model above, here told in generalities, but a rigorous treatment is not too much more involved, was used to estimate probabilities of joint defaults within pools of hundreds or even thousands of loans. The constant pairwise correlation is concerning, but the use of the normal distribution is even more so. Our previous analysis of the inability of the normal distribution to capture market extremes is apropos here as well.

The ease of implementation lead to this copula-based model here being used to mint huge numbers of triple-A rated bonds (made up from tiered levels of

pools of bonds). The pooled bonds were known as collateralized debt obligations, or CDOs. Concurrent with the acceptance of the modeling above, the CDO market grew from \$275 billion in 2000 to \$4.7 trillion in 2006.

Not only are correlations unstable (as we have already seen) and extreme events terribly likely, the CDO market (because of such great ratings by the ratings agencies) saw massive leverage. This was a recipe for disaster and culminated in the financial crisis of 2008. There were many people who could see this train-wreck coming far before it occurred, but in large part, the market did not. In effect, the market wasn’t efficient at pricing pairwise correlations; or, even worse, systemic crashes.

Li has apparently been unavailable for comment since the crisis.

Even with the above cautionary tale, we maintain that the powerful capability of modeling joint and marginal distributions separately is incomparable. Again, our emphasis is on interpretation and empiricism rather than on normative modeling.

As such, we may propose uses for the copula approach along the lines of Meucci [24], suggesting to use a joint Student *t* copula with five degrees of freedom and flexible marginals, or various panic copulas which reflect the stylized features of asymmetry and increased correlation in a crisis we have noted previously. Such additions are not merely refinements to the approach presented in Li’s paper, but well thought out and natural implementations of a flexible model in a market with a few known attributes. The copula modeling approach – much like the percentile modeling in VaR and CVaR interpretations of risk – allows the practitioner to articulate a wide variety of views. As such, its maligned history will likely be short.

### 3.3 Eigenvalues

We may further analyze the structure of the covariance matrix by studying its *eigenvalues and eigenvectors*. Recall that for a square matrix,  $A\in\mathbb{R}^{N\times N}$ , the scalar  $\lambda$  is an eigenvalue if

$$Av=\lambda v. \tag{3.20}$$

In this case, we say that the nonzero vector, *v*, is the eigenvector associated with  $\lambda$ . Notice that if *v* is an eigenvalue, then a scalar multiple, *cv*, satisfies

$$A(cv)=cAv=c\lambda v=\lambda(cv),$$

and hence we may assume without loss of generality that  $||v||=1$ .

Eigenvalues may be determined by considering that if

$$Av=\lambda v,$$

then

$$(A-\lambda I)v=0$$

where  $I$  is the identity matrix. This implies, then, that  $A - \lambda I$  is singular (i.e., it affords a nonzero solution to the above), and hence its determinant must be zero:

$$\det(A - \lambda I) = 0. \quad (3.21)$$

Equation (3.21) is known as the *characteristic equation*, and is a polynomial of degree  $N$ . We know that in  $\mathbb{C}$  such an equation will surely have all of its roots. We are interested in the case where all of the eigenvalues are real, and positive definiteness (even semidefiniteness) is a sufficient condition for just such a result. We will denote that  $A$  is positive definite by

$$A \succ 0$$

and positive semidefinite by

$$A \succeq 0.$$

**Theorem 3.3.1.** The eigenvalues of a positive semidefinite real matrix,  $A \succeq 0$ , are real and nonnegative. If  $A \succ 0$ , then the eigenvalues are strictly positive.

Proof. Let  $A \succeq 0$ , and let  $v$  be a (nonzero) eigenvector with associated eigenvalue  $\lambda$ . We have immediately that

$$Av = \lambda v,$$

and, premultiplying both sides by  $v'$ , we get

$$v'Av = \lambda v'v = \lambda.$$

Now, since  $A \succeq 0$ , the left hand side is nonnegative, and hence so is  $\lambda$ . The case where  $A \succ 0$  follows identically, resulting in strictly positive  $\lambda$ .

We have already seen that the covariance matrix,  $\Sigma$ , of the real valued multivariate random variable  $X$  is positive semidefinite; viz., for  $Y = w'X$ ,

$$\mathrm{Var}(Y) = w'\Sigma w.$$

A necessary and sufficient condition for  $\Sigma$ , then, is to ensure that the variance of  $Y$  is nonzero; i.e., that it is not the case that there exists a linear combination

$$\sum_{i=1}^{N} w_i X_i$$

that is zero in the quotient space of random variables identifying constants. In this case, we say that  $\{X_i\}_{i=1}^N$  is linearly independent. We have then that  $\Sigma = \mathrm{Cov}(X)$  is positive definite if and only if  $\{X_i\}_{i=1}^N$  is linearly independent.

Notice that in our previous work on the multivariate normal and Student  $t$  distributions, an implicit assumption was that  $\Sigma$  was invertible. We now may formulate this condition based on the linear independence of the components of  $X$ .

In a financial context, we may interpret an eigenvector,  $e_i$ , as a vector of portfolio weights or positions. Consider,

$$\begin{aligned}Var(e'_i X) &= e'_i \Sigma e_i \\ &= \lambda_i e'_i e_i \\ &= \lambda_i.\end{aligned}$$

So that  $\lambda_i$  is exactly the variance of the portfolio with positions  $e_i$ . We call

$$e'_i X \tag{3.22}$$

the  $i$ th principal component of  $X$ , and will often refer to both  $e_i$  and  $e'_i X$  as an *eigenportfolio*. The relative size of each eigenvalue is surprisingly meaningful and related to this observation and aids in problems of dimension reduction.

For a covariance matrix,  $\Sigma \in \mathbb{R}^{N \times N}$  with eigenvalues  $\{\lambda_i\}_{i=1}^N$  and associated eigenvectors  $\{e_i\}_{i=1}^N$  with

$$\lambda_1 \ge \cdots \ge \lambda_N \ge 0$$

we have that the eigenvectors of distinct eigenvalues are orthogonal; viz.,

$$e'_i e_j = 0$$

if  $\lambda_i \neq \lambda_j$ . To prove this, we note that

$$\Sigma e_i = \lambda_i e_i$$

and

$$\Sigma e_j = \lambda_j e_j$$

and so

$$\begin{aligned}e'_i \Sigma e_j &= \lambda_j e'_i e_j \\ e'_j \Sigma e_i &= \lambda_i e'_j e_i.\end{aligned}$$

Now  $e'_i \Sigma e_j = e'_j \Sigma e_i$  since  $\Sigma$  is symmetric. Therefore

$$\lambda_j e'_i e_j = \lambda_i e'_j e_i.$$

Similarly,  $e'_i e_j = e'_j e_i$ , giving

$$(\lambda_i - \lambda_j)e'_i e_j = 0.$$

Assuming that the eigenvalues of  $\Sigma$  are distinct, we may decompose the covariance matrix as

$$\Sigma = \sum_{i=1}^N \lambda_i e_i e'_i. \tag{3.23}$$

To see this, notice that

$$\begin{pmatrix} - & e_1 & - \\ \vdots & & \vdots \\ - & e_N & - \end{pmatrix} \begin{pmatrix} | & \cdots & | \\ e_1 & & e_N \\ | & & | \end{pmatrix} = \begin{pmatrix} e'_1 e_1 & \cdots & e'_1 e_N \\ \vdots & \ddots & \vdots \\ e'_N e_1 & \cdots & e'_N e_N \end{pmatrix} = I$$

by the orthogonality of eigenvectors of  $\Sigma$ . Now, for square matrices,  $A$  and  $B$  satisfying  $AB = I$ , we know (read: we leave to the reader) that  $BA = I$ . Hence

$$\begin{pmatrix} | & \cdots & | \\ e_1 & & e_N \\ | & & | \end{pmatrix} \begin{pmatrix} - & e_1 & - \\ \vdots & & \vdots \\ - & e_N & - \end{pmatrix} = I,$$

or

$$e_1 e'_1 + \cdots + e_N e'_N = I.$$

Finally, since

$$\Sigma e_i e'_i = \lambda_i e_i e'_i,$$

we have that

$$\begin{aligned} \Sigma &= \Sigma I \\ &= \Sigma \sum_i e_i e'_i \\ &= \sum_i \Sigma e_i e'_i \\ &= \sum_i \lambda_i e_i e'_i \end{aligned}$$

as desired.

As a result, utilizing the fact that the trace operator for a matrix is linear and the property that for square matrices  $A$  and  $B$ ,

$$\mathrm{tr}(AB) = \mathrm{tr}(BA),$$

we may relate the sum of variances of  $X$  to the sum of the eigenvalues of  $\Sigma$ . In particular, we have

$$\begin{aligned} \mathrm{tr}(\Sigma) &= \mathrm{tr} \left( \sum_i \lambda_i e_i e'_i \right) \\ &= \sum_i \lambda_i \mathrm{tr} (e_i e'_i) \\ &= \sum_i \lambda_i \mathrm{tr} (e'_i e_i) \\ &= \sum_i \lambda_i, \end{aligned}$$

or

$$\sum_i \sigma_i^2 = \sum_i \lambda_i. \tag{3.24}$$

We call  $\sum_i \lambda_i$  the *total variance* of  $\Sigma$ . In addition to relating the eigenvalues of  $\Sigma$  to the sum of variances, equation (3.24) also gives us a method for dimension reduction.

**Example 3.3.1.** Let  $X$  be an  $N$ -dimensional random vector representing the returns of  $N$  assets. For a threshold,  $\tau$ , with

$$0 < \tau \le 1$$

we may choose  $M$  eigenportfolios explaining  $\tau\%$  of the total variance by choosing the smallest  $M$  satisfying

$$\frac{\sum_{i=1}^M \lambda_i}{\sum_{i=1}^N \lambda_i} \ge \tau. \tag{3.25}$$

The related  $M$  eigenportfolios (or principal components),  $e'_i X$ , then comprise  $\tau\%$  of the total variance. This is especially effective for high dimensional  $X$  such as when considering the composition of the S&P 500, for example.

In Figure 3.3, an estimated covariance matrix was calculated monthly for a cross-section of the 50 largest stocks at the time by market cap. The covariance was calculated using 121 trailing weeks of returns. The largest  $N$  eigenvalues were chosen according to (3.25), with  $\tau = 80\%$ . A smoothed approximation, looking at the mean  $N_t$  for the trailing 18 months is shown as well.

Throughout, no more than 18 eigenportfolios were needed to explain more than 80% of the total variance. This is a significant decrease from the original dimension of 50. Additionally, the figure shows that there was a steeped decrease in the number of eigenportfolios ‘explaining the market’ from the financial crisis through 2015 – only rebounding from the lows sometime in 2011. This effect is likely related to the various Quantitative Easing programs initiated by the Fed at the time.

We may also ask how much of the total variance is explained by the eigenportfolio related to the largest eigenvalue. Figure 3.3 shows the time variation of the explanatory power of this eigenportfolio. We see again the significant upswing after the Financial Crisis, achieving levels of market coordination not seen in the preceding twenty years. While the adage that in a crisis correlations go to one is evidenced here (read the explanatory power of the largest eigenvalue increases significantly), the figure is also instructive, showing that the market is dynamic and that heretofore unseen influences like the Fed’s Qualitative Easing [12] can have significant and novel effects. However, we should note that these effects are interpretable – and perhaps even expected from the trained practitioner’s eye – using the mathematical edifice already in place and established here.

Carrying this type of reasoning further, one may posit that if a necessary (but clearly not sufficient) condition for a bear market downturn is an uptick in

![Line plot titled 'Number of Eigenvalues Needed for 80% Total Variance'. The Y-axis is 'Number of Eigenvalues' (ranging from 4 to 18) and the X-axis is 'Date' (ranging from 2000 to 2015). The plot shows a highly volatile blue line representing the number of eigenvalues needed, fluctuating significantly. An orange line, likely a smoothed version, shows a general trend. The number of eigenvalues peaked around 2007 (near 17) and dropped sharply around 2010 (to near 6), before recovering slightly.](3b6177b1edfdc8c7740f03f346d7838f_img.jpg)

Line plot titled 'Number of Eigenvalues Needed for 80% Total Variance'. The Y-axis is 'Number of Eigenvalues' (ranging from 4 to 18) and the X-axis is 'Date' (ranging from 2000 to 2015). The plot shows a highly volatile blue line representing the number of eigenvalues needed, fluctuating significantly. An orange line, likely a smoothed version, shows a general trend. The number of eigenvalues peaked around 2007 (near 17) and dropped sharply around 2010 (to near 6), before recovering slightly.

Figure 3.6: Plot of the number of eigenportfolios needed to account for 80% of total variance through time. Covariance is calculated using weekly returns over a trailing 121 week period with the largest 50 stocks by market cap evaluated monthly.

the explanatory power of the largest eigenvalue, a crisis post 2015 would require a break in the elevated levels seen since 2008. There is minor evidence of this in the figure as well.

Finally, we may look at the distribution of eigenvalues in a manner similar to our previous analysis of daily log returns for various stocks. Figure 3.3 shows the empirical density of the eigenvalues of the covariance matrix as before available on 12/31/2007. As with our discussion of the distribution of daily log returns, certain stylized features emerge.

Particularly, even with observations that are linearly independent, we see a peak of near-zero eigenvalues. In recent years, the tools of Random Matrix Theory (RMT) have been implemented in math finance to study this phenomenon. Authors like Bouchaud and Potters [4] present a methodology based on RMT to identify random, and hence noisy, eigenportfolios. Doing so seeks to modify the covariance (correlation) matrix to eliminate eigenportfolios with erroneously low contributions to risk. This effect is particularly important when considering mean-variance optimization.

In addition to a large bulk of eigenvalues clustering around zero, we also

![Line graph showing the percentage of total variance explained by the largest eigenvalue over time (Date, 2000 to 2012). The Y-axis is 'Percent of Total Variance' (0.2 to 0.6). The blue line represents the actual percentage, showing high volatility. The orange line represents a smoothed version of the percentage, which generally follows the trend of the blue line, peaking around 2010/2011 near 0.55 and dropping sharply after 2012.](65678100712a6817696109be76a51f0f_img.jpg)

**Percent of Total Variance Explained by Largest Eigenvalue**

Line graph showing the percentage of total variance explained by the largest eigenvalue over time (Date, 2000 to 2012). The Y-axis is 'Percent of Total Variance' (0.2 to 0.6). The blue line represents the actual percentage, showing high volatility. The orange line represents a smoothed version of the percentage, which generally follows the trend of the blue line, peaking around 2010/2011 near 0.55 and dropping sharply after 2012.

Figure 3.7: Plot of the percentage of total variance explained by the eigenportfolios associated with the largest eigenvalue through time. Covariance is calculated using weekly returns over a trailing 121 week period with the largest 50 stocks by market cap evaluated monthly.

note one very large eigenvalue – in this case, 2,745 times larger than the smallest eigenvalue, and 3.80 times larger than the second largest eigenvalue. This eigenvalue is documented as related to the market portfolio as in Avellaneda [1], but we cannot replicate these claims. However, we do note that the eigenportfolio for this eigenvalue very often has all positive entries (or, more accurately since eigenvectors are scalar independent, all entries share the same sign).

The covariance structure of equity returns, then, allows for a broad classification wherein the market is often driven by a dominant market portfolio, orders of magnitude larger than the smallest eigenvalue. This smallest eigenvalue, in turn, lies amongst a bulk of very near zero eigenvalues that may be classified in a technical sense as random noise. Further, the effects of the largest eigenvalue are interpretable in a dynamic sense, accounting for time specific features of the market as such.

![Empirical histogram of eigenvalues. The x-axis is Eigenvalue (scaled by 10^-2), ranging from 0 to 1.8. The y-axis is Frequency, ranging from 0 to 600. The distribution is highly concentrated near 0, with a frequency of approximately 520. Small frequencies are visible for eigenvalues up to 1.8, indicating a few large eigenvalues.](f117f4876cd1523b1717f3e38bd74229_img.jpg)

Empirical Distribution of Eigenvalues

Empirical histogram of eigenvalues. The x-axis is Eigenvalue (scaled by 10^-2), ranging from 0 to 1.8. The y-axis is Frequency, ranging from 0 to 600. The distribution is highly concentrated near 0, with a frequency of approximately 520. Small frequencies are visible for eigenvalues up to 1.8, indicating a few large eigenvalues.

Figure 3.8: Empirical histogram of the eigenvalues of the covariance matrix available on 12/31/2007. Covariance is calculated using weekly returns over a trailing 121 week period with the largest 50 stocks by market cap on that date.

### Exercises

1. Prove that for an inner product  $(\cdot, \cdot)$ , and for scalars  $\{a_i\}$  and  $\{b_j\}$ , and  $\{X_i\}$  and  $\{Y_j\}$  in  $V$ , we have

$$\left( \sum_i a_i X_i, \sum_j b_j Y_j \right) = \sum_i a_i \sum_j b_j (X_i, Y_j).$$

1. Verify that  $Cov(\cdot, \cdot)$  is symmetric and bilinear.
2. Prove that  $||\cdot||^2 = (\cdot, \cdot)$  defines a norm. In particular, this gives that variance is a norm on the quotient space of random variables.
3. Prove for univariate random variables  $X$  and  $Y$  with means  $\mu_X$  and  $\mu_Y$ , respectively, that

$$Cov(X, Y) = \mathbb{E}(XY) - \mu_X \mu_Y.$$

5. Using notation as in (2.43), show that

$$\hat{\sigma}_{XY} = \frac{1}{N-1} \sum_{i=1}^{N} (X_i - \hat{\mu}_X) (Y_i - \hat{\mu}_Y)$$

is an unbiased estimator of the covariance between  $X$  and  $Y$ .

6. For univariate random variables  $X$  and  $Y$ , and scalars,  $a$ ,  $b$ ,  $c$ , and  $d$ , if  $\text{Corr}(X, Y) = \rho$ , what is  $\text{Corr}(a + bX, c + dY)$ ?

7. Verify that  $f_U(x) = 1$  is the probability density function for uniformly distributed  $U$  on  $[0, 1]$ .

8. Use the change of variables theorem to determine the density for a uniformly distributed random variable on  $[a, b]$ .

9. Prove 3.18 for continuous  $F_i$  by looking at

$$\mathbb{P}(F_i(X_i) \le u)$$

for  $u \in [0, 1]$ .

10. Using the IBM/S&P daily return data, construct a copula function simulation with  $N = 1,000$  samples for the joint distribution of daily log returns where the joint distribution is Student  $t$  with five degrees of freedom with correlation matching the sample, and the marginals are also Student  $t$  with five degrees of freedom with means and volatilities matching the univariate sample means and standard deviations.

11. Using the cross-sectional and historical return data, plot the percentage of positive components of the eigenvector associated with the largest eigenvalue of the covariance through time; i.e., for eigenvector  $e_t = (e_{1t}, \dots, e_{Nt})$  at time  $t$ , plot

$$m_{it} = \frac{1}{N} \sum_{i=1}^{N} \delta_i(e_t)$$

where

$$\delta_i(e) = \begin{cases} 1 & \text{if } e_i > 0 \\ 0 & \text{otherwise.} \end{cases}$$

Use the same methodology as in the chapter, choosing the largest 50 companies by market cap at each time, and using the full 121 weeks of returns available. Why can we assume without loss of generality that  $m_{it} > 0.5$ ?

12. Prove that for square matrices  $A$  and  $B$  that if

$$AB = I$$

then

$$BA = I.$$

13. Consider the model

$$r = \alpha + \beta r_m + \epsilon$$

with  $r$ ,  $r_m$ , and  $\epsilon$  univariate random variables,

$$(r_m, \epsilon) = \text{Cov}(r_m, \epsilon) = 0,$$

and

$$\mathbb{E}(\epsilon) = 0.$$

Show that

$$\beta = \frac{\text{Cov}(r, r_m)}{\text{Var}(r_m)}$$

and

$$\alpha = \mathbb{E}(r) - \beta \mathbb{E}(r_m).$$

14. Prove that if the matrix  $A$  has eigenvalues  $\{\lambda_i\}_{i=1}^N$ , then  $A^k$  has eigenvalues  $\{\lambda_i^k\}_{i=1}^N$ .

15. Let  $(\cdot, \cdot)_F$  be defined for matrices (of appropriate dimensions) by  $(A, B)_F = \text{tr}(A'B)$ .

(a) Show that  $(\cdot, \cdot)_F$  is an inner product.

(b) For a covariance matrix,  $\Sigma$ , what is  $\sqrt{(\Sigma, \Sigma)_F}$  in terms of the eigenvalues of  $\Sigma$ ?

16. For the Euclidean vector norm  $||\cdot||$ ,

$$||x|| = (x'x)^{\frac{1}{2}},$$

define the matrix norm  $||\cdot||_2$  by

$$||A||_2 = \max_{||v||=1} ||Av||. \quad (3.26)$$

(a) Show that

$$||A||_2 = \max \frac{||Av||}{||v||}.$$

(b) Show that for any vector  $z$ ,

$$||Az|| \le ||A||_2 ||z||.$$

(c) Show

$$||AB||_2 \le ||A||_2 ||B||_2$$

(d) Show that for a positive definite matrix  $A$ ,

$$||A||_2 = \lambda_{\max},$$

where  $\lambda_{\max}$  is the maximum eigenvalue of  $A$ .

